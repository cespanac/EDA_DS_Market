{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria\n",
    "\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Este EDA tiene la finalidad de intentar entender el mercado laboral de Data Analyst y Data Scientist.\n",
    "\n",
    "Para esta ocasión se ha determinado buscar el dataset a través de la red social *LinkedIn*, creando una base de datos para posteriormente analizarla y sacar las conclusiones.\n",
    "\n",
    "### Etapas:\n",
    "\n",
    "Se han definido 4 etapas diferenciadas:\n",
    "\n",
    "1. **Web scraping**: para la recogida de datos se ha llevado a cabo la técnica de web scraping sobre LinkedIn con la librería de `Selenium` y utilizando como navegador `Chrome`. Hay dos notebooks documentados con las pruebas de como se ha llegado a la obtención. Y un tercero con la versión definitiva `WS_Linkedin`.\n",
    "\n",
    "    En resumen, consiste en buscar el empleo deseado en la red social y ejecutar el código sobre cada página. Contando que cada página tiene como unos 25 puestos. En total se han recogido 800 vacantes de cada tipo de puesto.\n",
    "    \n",
    "    Este método ha resultado complicado en el inicio por dos razones:\n",
    "    - Poca experiencia de uso con la librería *Selenium*.\n",
    "    - El código de LinkedIn es complejo de *scrapear*. Se ha quedado en un proceso semiautomático en el cual, se entra en la página de manera automática pero hay que hacer scroll para que el programa detecte todas las vacantes, así cómo cambiar de página. En cada página el programa recoge automáticamente toda la información.\n",
    "\n",
    "\n",
    "2. **Limpieza de datos**: una vez obtenidos los datos, debido a que el *scraping* no era 100 % efectivo, hubo que limpiar los datos. Está documentado en el notebook `Data_cleaning`.\n",
    "    - Cambiar los `na` por valores `no_data` o `0` en caso de int.\n",
    "    - Eliminar palabras irrelevantes como: *publicado hace* o *visualizaciones*.\n",
    "    - Limpiar las ubicaciones para su posterior uso, dejando solo la ciudad.\n",
    "    \n",
    "    2.1 En este caso se han presentado dificultades a la hora de aplicar una máscara con booleanos para que reconociese los `false` y los cambiase.\n",
    "\n",
    "\n",
    "3. **Análisis**: a partir de esta etapa, se ha tratado de analizar los diferentes tipos de datos. Debido a que casi todos los datos son cualitativos, se han realizado `counts` por agrupaciones y análisis de palabras concretas. Están recogidas en el notebook `Data_cleaning`.\n",
    "   \n",
    "    3.1 En primer lugar, se han contaneado los dos dataframes (D. Analyst y D. Scientist), añadiendo una última columna para describir el tipo de trabajo y de esa manera poder filtrarlos sobre un mismo dataframe.\n",
    "    \n",
    "    3.2 Aunque el dataframe ya estaba limpio, si hubo que aplicar cierto tratamiento a los datos. Por ejemplo, la columna de días activos se convirtió a fechas concretas para realizar un eje temporal. Aparte, se utilizó WorldCloud para poder aplicar un mapa de las palabras más utilizadas en los puestos ofertados y descripciones.\n",
    "    \n",
    "    3.2 Para hacer los gráficos se ha utilizado en su mayoría `plotly.express` debido a su compatibilidad con `Streamlit`. También se ha utilizado matplotlib para WorldCloud.\n",
    "\n",
    "\n",
    "4. **Presentación**: Por último, el desarrollo de la presentación se ha decidido hacerlo en `Streamlit`. Ya que ofrece una serie de características que se creía idóneo para la visualización del proyecto.\n",
    "\n",
    "    4.1 La más importante, el poder compartir el proyecto en una página a la que otras personas puedan acceder e interactuar.\n",
    "    \n",
    "    4.2 Separación por pestañas: dividiendo el trabajo por secciones, haciéndolo más dinámico.\n",
    "    \n",
    "    4.3 Uso de mapa para representar las ubicaciones de los puestos de trabajo.\n",
    "    \n",
    "    4.4 Filtrado del dataframe para comprobar cualquier tipo de empresa, puesto ofertado, ubicación, nivel de experiencia o nº de solicitudes.\n",
    "\n",
    "    4.5 Todo el código de *Streamlit* ha sido desarrollado en el IDE PyCharm CE. Se han utilizado 3 scripts:\n",
    "    - *main*: código de funcionamiento de la página y visualización.\n",
    "    - *funciones*: funciones de uso de la página, como configuración de la página y los filtros. \n",
    "    - *gráficos*: gráficos cargados en funciones a partir del código desarrollado en el notebook.\n",
    "\n",
    "### Conclusiones\n",
    "\n",
    "Ha sido un proyecto muy interesante debido a que se han utilizado muchas de las herramientas aprendidas durante el bootcamp: bucles, funciones, numpy, pandas, data sources, visualización, feature engineering y estadística descriptiva.\n",
    "\n",
    "De esta manera, se han podido integrar todos estos conocimientos y ponerlos en práctica en un caso real, en el que también hay un objetivo de aportar un mayor conocimiento a los estudiantes de la escuela sobre el mercado laboral para el que se están formando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
